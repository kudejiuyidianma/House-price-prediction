# House Price Prediction

```{r}
### Import libraries
library(randomForest)
library(ggplot2)


```

## Clean
```{r}
house <- read.csv('/Users/miku/Documents/SBU/Term 5/AMS 380/Real estate valuation data set(1).csv')
house <- subset(house, select = -c(No) )
head(house)
```

```{r}
summary(house)
```



```{r}
boxplot(house$X1.transaction.date)
```

```{r}
boxplot(house$X2.house.age)
```

```{r}
boxplot(house$X3.distance.to.the.nearest.MRT.station)
```

```{r}
boxplot(house$X4.number.of.convenience.stores)
```

```{r}
boxplot(house$X5.latitude)
```

```{r}
boxplot(house$X6.longitude)
```

```{r}
splitSample <- sample(1:3, size=nrow(house), prob=c(0.7,0.15,0.15), replace = TRUE)
train.data <- house[splitSample==1,]
valid.data <- house[splitSample==2,]
test.data <- house[splitSample==3,]
```



```{r}
set.seed(4543)
rf.fit <- randomForest(Y.house.price.of.unit.area ~ ., data=train.data, ntree=1000,
                       keep.forest=FALSE, importance=TRUE)
```

```{r}
rf.fit
```


The mean of squared residuals and % variance explained indicate how well the model fits the data. Residuals are a difference between prediction and the actual value. In our example, 52.24819 means that we were wrong by 52.24819 on average. 
```{r}
y_pred <- predict(rf.fit, test.data[-6])
```

```{r}
ImpData <- as.data.frame(importance(rf.fit))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```
Random forest regression in R provides two outputs: decrease in mean square error (MSE) and node purity. Prediction error described as MSE is based on permuting out-of-bag sections of the data per individual tree and predictor, and the errors are then averaged. In the regression context, Node purity is the total decrease in residual sum of squares when splitting on a variable averaged over all trees (i.e. how well a predictor decreases variance). MSE is a more reliable measure of variable importance. If the two importance metrics show different results, listen to MSE. 
